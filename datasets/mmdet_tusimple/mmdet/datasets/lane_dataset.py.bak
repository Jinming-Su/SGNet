import os.path as osp
import os
import cv2
import mmcv
import numpy as np
from torch.utils.data import Dataset
from mmdet.core import eval_map, eval_lane_recalls
from .builder import DATASETS
from .custom import CustomDataset
from .pipelines import Compose
from mmdet.core.bbox.anchorandrect import *
@DATASETS.register_module()
class LaneDataset(CustomDataset):
    """Custom dataset for detection.

        The annotation format is shown as follows. The `ann` field is optional for
        testing.

        .. code-block:: none

            [
                {
                    'filename': 'a.jpg',
                    'width': 1280,
                    'height': 720,
                    'ann': {
                        'bboxes': <np.ndarray> (n, 4) in (x1, y1, x2, y2) order.
                        'labels': <np.ndarray> (n, ),
                        'bboxes_ignore': <np.ndarray> (k, 4), (optional field)
                        'labels_ignore': <np.ndarray> (k, 4) (optional field)
                    }
                },
                ...
            ]

        Args:
            ann_file (str): Annotation file path.
            pipeline (list[dict]): Processing pipeline.
            classes (str | Sequence[str], optional): Specify classes to load.
                If is None, ``cls.CLASSES`` will be used. Default: None.
            data_root (str, optional): Data root for ``ann_file``,
                ``img_prefix``, ``seg_prefix``, ``proposal_file`` if specified.
            test_mode (bool, optional): If set True, annotation will not be loaded.
            filter_empty_gt (bool, optional): If set true, images without bounding
                boxes will be filtered out.
        """
    CLASSES = ('lane', )
    def __init__(self, ann_file, pipeline, img_dir, ann_dir, test_mode=False):
        self.img_dir = img_dir
        self.ann_dir = ann_dir
        #存储图片的txt的文件
        self.ann_file =ann_file
        self.pipeline = pipeline
        self.test_mode = test_mode
        # load annotations (and proposals)
        self.data_infos = self.load_annotations(ann_file)
        # set group flag for the sampler
        # processing pipeline
        self.pipeline = Compose(pipeline)
        # set group flag for the sampler
        if not self.test_mode:
            self._set_group_flag()
        self.proposals = None


    def load_annotations(self, ann_file):
        img_infos = []
        img_ids = mmcv.list_from_file(ann_file)
        for img_id in img_ids:
            filename = '{}.jpg'.format(img_id)
            txt_path = osp.join(self.ann_dir, '{}.txt'.format(img_id))
            ann_content = mmcv.list_from_file(txt_path)
            line=[]
            bboxes = []
            labels = []
            if ann_content:
                line = ann_content[0].strip('\n').split(',')
                line = list(map(float, line))
                bboxes = []
                labels = []
                for bbox_anno in ann_content[1:]:
                    bbox = bbox_anno.strip('\n').split(',')
                    bbox = list(map(float, bbox))
                    bboxes.append(bbox[:3] + bbox[-1:])  #暂时不load h
                    labels.append(0)
            if not bboxes:
                line = np.zeros(2)
                bboxes = np.zeros((0, 4))
                labels = np.zeros((0, ))
            else:
                line = np.array(line)
                bboxes = np.array(bboxes)
                labels = np.array(labels)
            img_infos.append(dict(id=img_id, filename=filename, width=590, height=1640, ann=dict(
                line=line.astype(np.float32) ,
                bboxes=bboxes.astype(np.float32),
                labels=labels.astype(np.int64)
            )))
                # with open(txt_file, 'r+') as f:
                #     ann_contend = f.readlines()
                #     if not line:
                #         ann = None
                #     else:
                #         line = line.strip('\n').split(',')
                #         line = list(map(float, line))
                #         self.anno_list.append(line)

        return img_infos
        # for img_file in tqdm(self.img_list):
        #     frame_mp4_number = os.sep.join(img_file.split(os.sep)[-3:])[:-4]
        #     anno_file = os.path.join(self.cfg.anno_dir, frame_mp4_number + '.txt')
        #     with open(anno_file, 'r+') as f:
        #         line = f.readline()
        #         if not line:
        #             self.anno_list.append([-1, -1])
        #         else:
        #             line = line.strip('\n').split(',')
        #             line = list(map(float, line))
        #             self.anno_list.append(line)
    
    
    def pre_pipeline(self, results):
        """Prepare results dict for pipeline."""
        results['img_prefix'] = self.img_dir
        results['bbox_fields'] = []
        results['line_fields'] = []
    
    
    def prepare_train_img(self, idx):
        """Get training data and annotations after pipeline.

        Args:
            idx (int): Index of data.

        Returns:
            dict: Training data and annotation after pipeline with new keys \
                introduced by pipeline.
        """
        img_info = self.data_infos[idx]
        ann_info = self.get_ann_info(idx)
        results = dict(img_info=img_info, ann_info=ann_info)
        if self.proposals is not None:
            results['proposals'] = self.proposals[idx]
        self.pre_pipeline(results)
        return self.pipeline(results)

    def prepare_test_img(self, idx):
        """Get testing data  after pipeline.

        Args:
            idx (int): Index of data.

        Returns:
            dict: Testing data after pipeline with new keys intorduced by \
                piepline.
        """

        img_info = self.data_infos[idx]
        results = dict(img_info=img_info)
        if self.proposals is not None:
            results['proposals'] = self.proposals[idx]
        self.pre_pipeline(results)
        return self.pipeline(results)
    
    def __getitem__(self, idx):
        """Get training/test data after pipeline.

        Args:
            idx (int): Index of data.

        Returns:
            dict: Training/test data (with annotation if `test_mode` is set \
                True).
        """
        if self.test_mode:
            return self.prepare_test_img(idx)
        while True:
            data = self.prepare_train_img(idx)
            if data is None:
                idx = self._rand_another(idx)
                continue
            return data
        
    def evaluate(self,
                 results,
                 metric='mAP',
                 logger=None,
                 proposal_nums=(30, 50, 100),
                 iou_thr=(0.8, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99),
                 scale_ranges=None):
        """Evaluate the dataset.

        Args:
            results (list): Testing results of the dataset.
            metric (str | list[str]): Metrics to be evaluated.
            logger (logging.Logger | None | str): Logger used for printing
                related information during evaluation. Default: None.
            proposal_nums (Sequence[int]): Proposal number used for evaluating
                recalls, such as recall@100, recall@1000.
                Default: (100, 300, 1000).
            iou_thr (float | list[float]): IoU threshold. It must be a float
                when evaluating mAP, and can be a list when evaluating recall.
                Default: 0.5.
            scale_ranges (list[tuple] | None): Scale ranges for evaluating mAP.
                Default: None.
        """
        results_line = results[0]
        results_proposal = results[1]
        import pdb
        pdb.set_trace()
        if not isinstance(metric, str):
            assert len(metric) == 1
            metric = metric[0]
        allowed_metrics = ['mAP', 'recall']
        if metric not in allowed_metrics:
            raise KeyError(f'metric {metric} is not supported')
        annotations = [self.get_ann_info(i) for i in range(len(self))]
        eval_results = {}
        if metric == 'mAP':
            assert isinstance(iou_thr, float)
            mean_ap, _ = eval_map(
                results_proposal,
                annotations,
                scale_ranges=scale_ranges,
                iou_thr=iou_thr,
                dataset=self.CLASSES,
                logger=logger)
            eval_results['mAP'] = mean_ap
        
        elif metric == 'recall':
            gt_bboxes = [ann['bboxes'] for ann in annotations]
            if isinstance(iou_thr, float):
                iou_thr = [iou_thr]
            recalls = eval_lane_recalls(
                gt_bboxes, results_proposal, proposal_nums, iou_thr, logger=logger, img_shape=(590, 1640))
            for i, num in enumerate(proposal_nums):
                for j, iou in enumerate(iou_thr):
                    eval_results[f'recall@{num}@{iou}'] = recalls[i, j]
            if recalls.shape[1] > 1:
                ar = recalls.mean(axis=1)
                for i, num in enumerate(proposal_nums):
                    eval_results[f'AR@{num}'] = ar[i]
        return eval_results
    
    def find_low_recall(self,
                 results,
                 metric='mAP',
                 logger=None,
                 proposal_nums=50,
                 iou_thr=0.95,
                 scale_ranges=None,
                 out_dir='./tmp/low_recall/'):
        """Evaluate the dataset.

        Args:
            results (list): Testing results of the dataset.
            metric (str | list[str]): Metrics to be evaluated.
            logger (logging.Logger | None | str): Logger used for printing
                related information during evaluation. Default: None.
            proposal_nums (Sequence[int]): Proposal number used for evaluating
                recalls, such as recall@100, recall@1000.
                Default: (100, 300, 1000).
            iou_thr (float | list[float]): IoU threshold. It must be a float
                when evaluating mAP, and can be a list when evaluating recall.
                Default: 0.5.
            scale_ranges (list[tuple] | None): Scale ranges for evaluating mAP.
                Default: None.
        """
        results_line = results[0]
        results_proposal = results[1]
        print('proposal_nums {}, iou_thr {}, write to {}'.format(proposal_nums, iou_thr, out_dir))
        seg_label_dir = '/home/hadoop-mtcv/cephfs/data/chenchao60/CULane/laneseg_label_w16/'
        img_dir = '/workdir/chenchao/data/CULane/'
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
        if not isinstance(metric, str):
            assert len(metric) == 1
            metric = metric[0]
        allowed_metrics = ['mAP', 'recall']
        if metric not in allowed_metrics:
            raise KeyError(f'metric {metric} is not supported')
        annotations = [self.get_ann_info(i) for i in range(len(self))]
        eval_results = {}
        gt_bboxes = [ann['bboxes'] for ann in annotations]
        if isinstance(iou_thr, float):
            iou_thr = [iou_thr]
        low_recall_list = []
        # iou和proposal num设为1个
        for i in range(len(self)):
            recalls = eval_lane_recalls(
                gt_bboxes[i:i+1], results_proposal[i:i+1], proposal_nums, iou_thr, logger=logger, img_shape=(590, 1640), show_recall=False)
            if recalls[0][0] != 1 and (not np.isnan(recalls[0][0])):
                img_id = self.data_infos[i]['id']
                print('recalls', recalls, img_id)
                low_recall_list.append(i)
        print('len', len(low_recall_list))    
        for img_i in low_recall_list:
            img_id = self.data_infos[img_i]['id']
            result = results_proposal[img_i]
            pred_line = results_line[img_i]
            img_file = os.path.join(img_dir, img_id + '.jpg')
            img = cv2.imread(img_file)
            seg_img_file = os.path.join(seg_label_dir, img_id + '.png')
            img_id_2  = '_'.join(img_id.split(os.sep))
            seg_img = cv2.imread(seg_img_file) * 50
            anno_line = self.get_ann_info(img_i)['line']
            H = seg_img.shape[0]
            W = seg_img.shape[1]
            cv2.line(seg_img, (0, int(anno_line[0])), (W-1, int(anno_line[1])), (0, 255, 255), 2)
            cv2.line(seg_img, (0, int(H * pred_line[0])), (W-1, int(H * pred_line[1])), (255, 0, 0), 2)
            #result = result[result[:, 4]>score_thr]
            sort_idx = np.argsort(result[:, 4])[::-1][:proposal_nums]
            result = result[sort_idx]
            result_rects = anchor2rect_mask_np(result)
            for rect_i, rect in enumerate(result_rects): 
                score = result[rect_i][4]
                draw_rot_rect(rect, seg_img)
                cv2.putText(seg_img, str(score), (int(rect[0]), int(rect[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1)
            stack_img = np.vstack((seg_img, img))
            cv2.imwrite(os.path.join(out_dir ,  img_id_2 + '.jpg'), stack_img) 
#         if isinstance(iou_thr, float):
#             iou_thr = [iou_thr]
        
#         if metric == 'mAP':
#             assert isinstance(iou_thr, float)
#             mean_ap, _ = eval_map(
#                 results,
#                 annotations,
#                 scale_ranges=scale_ranges,
#                 iou_thr=iou_thr,
#                 dataset=self.CLASSES,
#                 logger=logger)
#             eval_results['mAP'] = mean_ap
#         elif metric == 'recall':
#             gt_bboxes = [ann['bboxes'] for ann in annotations]
#             if isinstance(iou_thr, float):
#                 iou_thr = [iou_thr]
#             recalls = eval_lane_recalls(
#                 gt_bboxes, results, proposal_nums, iou_thr, logger=logger, img_shape=(590, 1640))
#             for i, num in enumerate(proposal_nums):
#                 for j, iou in enumerate(iou_thr):
#                     eval_results[f'recall@{num}@{iou}'] = recalls[i, j]
#             if recalls.shape[1] > 1:
#                 ar = recalls.mean(axis=1)
#                 for i, num in enumerate(proposal_nums):
#                     eval_results[f'AR@{num}'] = ar[i]
#         return eval_results
    
    def visualize_rpn(self,
                 results,
                 logger=None,
                 proposal_nums=200,
                 score_thr=0.0,
                 scale_ranges=None,
                 out_dir = '/workdir/chenchao/projects/mmdetection-master/result_vis/'):
        results_line = results[0]
        results_proposal = results[1]
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
        print('score_thr {}, proposal_nums {}, write to {}'.format(score_thr, proposal_nums, out_dir))
        seg_label_dir = '/home/hadoop-mtcv/cephfs/data/chenchao60/CULane/laneseg_label_w16/'
        img_dir = '/workdir/chenchao/data/CULane/'
        assert len(results_proposal) == len(self)
        for img_i in range(len(self)):
            result = results_proposal[img_i]
            pred_line = results_line[img_i]
            img_id = self.data_infos[img_i]['id']
            img_file = os.path.join(img_dir, img_id + '.jpg')
            img = cv2.imread(img_file)
            seg_img_file = os.path.join(seg_label_dir, img_id + '.png')
            anno_line = self.get_ann_info(img_i)['line']
            img_id_2  = '_'.join(img_id.split(os.sep))
            seg_img = cv2.imread(seg_img_file) * 50
            H = seg_img.shape[0]
            W = seg_img.shape[1]
            cv2.line(seg_img, (0, int(anno_line[0])), (W-1, int(anno_line[1])), (0, 255, 255), 2)
            cv2.line(seg_img, (0, int(H * pred_line[0])), (W-1, int(H * pred_line[1])), (255, 0, 0), 2)
            #result = result[result[:, 4]>score_thr]
            sort_idx = np.argsort(result[:, 4])[::-1][:proposal_nums]
            result = result[sort_idx]
            result_rects = anchor2rect_mask_np(result)
            for rect_i, rect in enumerate(result_rects): 
                score = result[rect_i][4]
                draw_rot_rect(rect, seg_img)
                cv2.putText(seg_img, str(score), (int(rect[0]), int(rect[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1)
            stack_img = np.vstack((seg_img, img))
            cv2.imwrite(os.path.join(out_dir ,  img_id_2 + '.jpg'), stack_img) 
    
    def visualize_rcnn(self,
                 results,
                 logger=None,
                 proposal_nums=200,
                 score_thr=0.0,
                 scale_ranges=None,
                 out_dir = '/workdir/chenchao/projects/mmdetection-master/tmp/result_faster_rcnn/'):
        results_line = results[0]
        results_proposal = results[1]
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
        print('score_thr {}, proposal_nums {}, write to {}'.format(score_thr, proposal_nums, out_dir))
        seg_label_dir = '/home/hadoop-mtcv/cephfs/data/chenchao60/CULane/laneseg_label_w16/'
        img_dir = '/workdir/chenchao/data/CULane/'
        assert len(results_proposal) == len(self)
        for img_i in range(len(self)):
            result = results_proposal[img_i][0]
            pred_line = results_line[img_i]
            img_id = self.data_infos[img_i]['id']
            img_file = os.path.join(img_dir, img_id + '.jpg')
            img = cv2.imread(img_file)
            seg_img_file = os.path.join(seg_label_dir, img_id + '.png')
            anno_line = self.get_ann_info(img_i)['line']
            img_id_2  = '_'.join(img_id.split(os.sep))
            seg_img = cv2.imread(seg_img_file) * 50
            H = seg_img.shape[0]
            W = seg_img.shape[1]
            cv2.line(seg_img, (0, int(anno_line[0])), (W-1, int(anno_line[1])), (0, 255, 255), 2)
            cv2.line(seg_img, (0, int(H * pred_line[0])), (W-1, int(H * pred_line[1])), (255, 0, 0), 2)
            result = result[result[:, 4]>score_thr]
            sort_idx = np.argsort(result[:, 4])[::-1][:proposal_nums]
            result = result[sort_idx]
            result_rects = anchor2rect_mask_np(result)
            for rect_i, rect in enumerate(result_rects): 
                score = result[rect_i][4]
                draw_rot_rect(rect, seg_img)
                cv2.putText(seg_img, str(score), (int(rect[0]), int(rect[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1)
            stack_img = np.vstack((seg_img, img))
            cv2.imwrite(os.path.join(out_dir ,  img_id_2 + '.jpg'), stack_img) 
        
